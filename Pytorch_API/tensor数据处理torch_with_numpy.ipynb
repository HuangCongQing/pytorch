{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "TODO\n",
    "\n",
    "Tips:\n",
    "* 有时候tensor需要转到numpy进行处理\n",
    "\n",
    "https://www.yuque.com/huangzhongqing/pytorch/gck7a0#C4vQj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.tensor([\n",
    "    [1,2,3,],\n",
    "    [4,5,6]\n",
    "], dtype=torch.float32)\n",
    "print(a)\n",
    "print(np.arange(12))  # [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
    "\n",
    "x=torch.arange(12).view(3,4)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np和tensor的互换\n",
    "1. x.numpy()\n",
    "2. torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(2,2)\n",
    "x1 = x.numpy() # torch转换到numpy\n",
    "x2 = torch.from_numpy(x1) #numpy转换torch   torch.from_numpy(ndarray)--->Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维度交换 transpose()和permute()\n",
    "1. 交换维度 分别时transpose()和permute()\n",
    "2. 交换维度内的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5, 3, 4])\n",
      "(1, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# 该函数可以交换tensor的任意两个维度，即一次只能交换两个维度\n",
    "import torch\n",
    "x = torch.randn(8, 3, 5, 4)\n",
    "y = x.transpose(1,2)  # 交换第二与第三维度\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 类别numpy的transpose\n",
    "## 应用 xyz变换\n",
    "points = torch.randn(1, 3, 256).cuda()\n",
    "final_xyz = points.detach().cpu().numpy() #  最后采样点 256\n",
    "final_xyz = np.transpose(final_xyz, (0,2,1)) # 交换维度\n",
    "print(final_xyz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5, 3, 4])\n",
      "torch.Size([4, 8, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 该函数可以随意交换任意维度，并且可以重新排列整合维度\n",
    "\n",
    "z1 = x.permute(0,2,1,3)  # 交换第二与第三维度\n",
    "print(z1.shape)\n",
    "z2 = x.permute(3,0,2,1)  # 对原维度重新排列整合\n",
    "print(z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交换维度内的列\n",
    "# torch.ones(4,24513,4) torch.zeros(4,134343,4)\n",
    "gt_bboxes = torch.randn(6,8) #  (label,xyzlwl.r)# (1,6,8)# \n",
    "gt_bboxes = gt_bboxes[:,[7,0,1,2,3,4,5,6]] # 交换列\n",
    "# print(gt_bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼接torch.cat(=np.concatenate) &torch.stack(=np.stack())\n",
    "torch.cat &torch.stack(sequence, dim=0, out=None)，\n",
    "#注: .cat 和 .stack的区别在于 cat会增加现有维度的值(行列数量),可以理解为续接，stack会新加增加一个维度\n",
    "\n",
    "* torch.cat(seq,dim=0,out=None) # 沿着dim连接seq中的tensor, 所有的tensor必须有相同的size或为empty， 其相反的操作为 torch.split() 和torch.chunk()\n",
    "* torch.stack(seq, dim=0, out=None) #同上\n",
    "\n",
    ">>> a=torch.Tensor([1,2,3])\n",
    ">>> torch.stack((a,a)).size()\n",
    "torch.size(2,3)\n",
    ">>> torch.cat((a,a)).size()\n",
    "torch.size(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "tensor([[-0.2273, -0.4295,  1.4026, -2.3180,  1.7877,  0.9202,  0.1598,  1.0000],\n",
      "        [-1.7302,  0.2141, -0.6040,  0.4102,  0.8396,  0.4340,  1.3516,  1.0000],\n",
      "        [ 1.0719, -0.3730,  0.7672,  0.4611, -0.4092, -0.2790, -0.8011,  1.0000],\n",
      "        [ 0.3402, -0.3748, -1.1944,  1.8757, -0.5044, -0.5347, -0.1162,  1.0000],\n",
      "        [-0.6060,  1.5955,  0.1573,  0.1331,  0.3430,  0.0413,  1.4608,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# result = torch.cat([pred_bbox[0,:,:], pred_label[0,:,:]], dim=1) \n",
    "pred_bbox = torch.randn(1, 32, 7)\n",
    "pred_label = torch.ones(1,32,1) #把0去掉或者reshape[0,:,:]\n",
    "result = torch.cat([pred_bbox[0,:,:], pred_label[0,:,:]], dim=1) # \n",
    "print(result.shape)\n",
    "print(result[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  1,   2,   3],\n",
      "         [  4,   5,   6]],\n",
      "\n",
      "        [[ 10,  20,  30],\n",
      "         [ 40,  50,  60]],\n",
      "\n",
      "        [[100, 200, 300],\n",
      "         [400, 500, 600]]])\n",
      "tensor([[[  1,   2,   3],\n",
      "         [ 10,  20,  30],\n",
      "         [100, 200, 300]],\n",
      "\n",
      "        [[  4,   5,   6],\n",
      "         [ 40,  50,  60],\n",
      "         [400, 500, 600]]])\n",
      "tensor([[[  1,  10, 100],\n",
      "         [  2,  20, 200],\n",
      "         [  3,  30, 300]],\n",
      "\n",
      "        [[  4,  40, 400],\n",
      "         [  5,  50, 500],\n",
      "         [  6,  60, 600]]])\n",
      "torch.Size([3, 2, 3])\n",
      "torch.Size([2, 3, 3])\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[  1,   2,   3,  10,  20,  30, 100, 200, 300],\n",
      "        [  4,   5,   6,  40,  50,  60, 400, 500, 600]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor([[1,2,3],\n",
    "                [4,5,6]])  # 2x3\n",
    "b=torch.tensor([[10,20,30],\n",
    "                [40,50,60]])\n",
    "c=torch.tensor([[100,200,300],\n",
    "                [400,500,600]])\n",
    "print(torch.stack([a,b,c],dim=0)) # 0维\n",
    "print(torch.stack([a,b,c],dim=1)) # \n",
    "print(torch.stack([a,b,c],dim=2)) # 将每个tensor的第i行转置后按列连接组成一个新的2维tensor，再将这些新tesnor按照dim=0的方式连接\n",
    "print(torch.stack((a,b,c),dim=0).size())\n",
    "print(torch.stack([a,b,c],dim=1).size())\n",
    "print(torch.stack([a,b,c],dim=2).size())\n",
    "print(torch.cat((a,b,c), dim=-1))\n",
    "\n",
    "\n",
    "# dim=0时，将tensor在一维上连接，简单来说就是，就是将tensor1，tensor2…tensor n,连接为【tensor1，tensor2… tensor n】（就是在这里产生了扩维）\n",
    "# dim=1时，将每个tensor的第i行按行连接组成一个新的2维tensor，再将这些新tensor按照dim=0的方式连接。\n",
    "# dim=2时，将每个tensor的第i行转置后按列连接组成一个新的2维tensor，再将这些新tesnor按照dim=0的方式连接\n",
    "# 原文链接：https://blog.csdn.net/realcoder/article/details/105846408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维度增减unsqueeze是squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1个维度torch.squeeze(inout,dim=None,out=None) &torch.unsqueeze() \n",
    "* torch.squeeze() 对于tensor变量进行维度压缩，去除维数为1的的维度\n",
    "* unsqueeze是squeeze()的反向操作，增加一个维度，该维度维数为1，可以指定添加的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([1, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "out = torch.arange(12).reshape(1,3,4)\n",
    "print(out.shape)\n",
    "print(torch.squeeze(out).shape)\n",
    "print(torch.unsqueeze(out, dim=0).shape) # 在0维之前插入这一维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛选满足条件的行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]]) tensor([False,  True,  True])\n",
      "tensor([[ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# lmv/core/trainers.py\n",
    "# 筛选第一列不等于0\n",
    "gt_bboxes =torch.arange(12).view(3,4)\n",
    "print(gt_bboxes,gt_bboxes[:,0] != 0) # tensor([False,  True,  True]) 每行第1列（label）不等于0\n",
    "gt_bboxes = gt_bboxes[gt_bboxes[:,0] != 0, :] # 第1列（label）不等于0\n",
    "print(gt_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4]]) torch.Size([1, 5])\n",
      "tensor([[False, False, False,  True,  True]])\n",
      "tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(5).reshape(1,5)\n",
    "print(x,x.shape)\n",
    "print(x>2) # tensor([[False, False, False,  True,  True]])\n",
    "print(x[x>2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引（index_select()。 masked_select()）\n",
    "https://blog.csdn.net/Env1sage/article/details/124719553?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  torch.gather/scatter_ 聚集/分散操作\n",
    "\n",
    "https://www.yuque.com/huangzhongqing/pytorch/gck7a0#BziNU\n",
    "\n",
    "一般scatter用于生成onehot向量。https://blog.csdn.net/qq_39004117/article/details/95665418\n",
    "\n",
    "`torch.Tensor.scatter_(dim, index, src) → Tensor`\n",
    "\n",
    "字面意思：对一个 torch.Tensor 进行操作，dim，index，src三个为输入的参数。\n",
    "\n",
    "* dim 就是在哪个维度进行操作，注意，dim 的不同，在其他条件相同的条件下得到的output 也不同。\n",
    "* index 是输入的索引。\n",
    "* src 就是输入的向量，也就是 input。\n",
    "相关：/home/chongqinghuang/code/lmv/core/models/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "# input = torch.ones(2, 4)\n",
    "input = 1\n",
    "print(input)\n",
    "output = torch.zeros(3, 5)\n",
    "print(output)\n",
    "index = torch.tensor([[1], [2], [3]]) # 在不同的下标位置赋值1\n",
    "output = output.scatter(1, index, input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e39c63690641fda45a9b5b3a54295d3c7c7609e6d639cc54d178959f811fe3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pcdet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a4bb52955cb4b7ba3167560f52fbe8257746766a280283c0daccbe36f0c9125"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
